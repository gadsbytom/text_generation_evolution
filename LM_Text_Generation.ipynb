{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models for text generation\n",
    "* One traditional approach - Markov Chains\n",
    "* And two deep approaches - LSTMs and Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "Text generation methods have evolved over time. Markov chains, popular from the mid 20th century to 2000s (cf Theo Lutz), captured local dependencies but lacked long-range understanding. LSTMs were developed in 1997, and gained popularity through the late 2000s and 2010s. They addressed long-range dependencies but they are slow, and, despite their name, have trouble with long term memory. Transformers, emerging in 2017, leveraged attention mechanisms for global understanding but required significant computational resources. The popularity shifted to newer methods due to the need for improved text quality and practical considerations. These days, Transformers rule the roost."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The objective is to create a model which tries to assess the liklehood of language:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w_{t+1} | w_{t-1+n}, ..., w_{t})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional approach - trigram Markov Chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for a given dataset\n",
    "* bin the words into size 2 unique pairs\n",
    "* for a given pair, find the succeeding word\n",
    "* build a transition probability matrix (markov chain) of these relationships\n",
    "* use a sampler on this matrix to generate new text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(remove=['headers', 'footers'])\n",
    "text = data['data']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the trigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_word_distribution(data):\n",
    "    \"\"\"create a probability distribution over all trigrams\n",
    "    \n",
    "        params: data - a Bunch data object from sklearn\n",
    "        returns: [Bigram probability distribution, trigram probability distribution]\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = ' '.join([' '.join(re.findall('(?u)\\\\b\\\\w\\\\w*\\\\b',article.lower())) for article in text]).split()\n",
    "    tri_gram = [' '.join([x,y]) for x,y in zip(all_data[:-1:], all_data[1::])]\n",
    "    next_word = all_data[2:] + [' '] * 1\n",
    "    words = pd.DataFrame({'seed_word':all_data[:-1],'gram_words':tri_gram, \"next_word\":next_word})\n",
    "    words['seed_next_word'] = words['seed_word'].shift(-1)\n",
    "    seed_word_distribution = words.groupby('seed_word')['seed_next_word'].value_counts(normalize=True)\n",
    "    gram_word_distribution = words.groupby('gram_words')['next_word'].value_counts(normalize=True)\n",
    "    \n",
    "    return [seed_word_distribution, gram_word_distribution]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = trigram_word_distribution(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_text_generation(seed, length, distribution):\n",
    "    \"\"\"seed a distribution with a seed word, and ask it to make more words\n",
    "        \n",
    "        params: seed - A seed word, \n",
    "                length -Length of the generated sentence\n",
    "                distribution - A word probability distribution\n",
    "                \n",
    "        returns: generated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        seed = seed.lower()\n",
    "        seed += ' ' + np.random.choice(distribution[0][seed].index, p=distribution[0][seed].values)\n",
    "        for i in range(length):\n",
    "             seed += ' ' + np.random.choice(distribution[1][' '.join(seed.split()[-2:])].index, p=distribution[1][' '.join(seed.split()[-2:])].values)\n",
    "        return seed\n",
    "    \n",
    "    except:\n",
    "        print('Oops! Try another word as seed word')\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'It'\n",
    "sentence_length = 20\n",
    "sentence = trigram_text_generation(seed,sentence_length, dist)\n",
    "sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks:\n",
    "\n",
    "* It is still imperfect at capturing anything above basic syntax, and has no semantics or pragmatic capability\n",
    "* A quad-gram would be better, but as we increase gram size, transition matrices require increasingly more computation power to train, and space to store.\n",
    "* N-grams are a sparse representation of language -  any word not present in the training corpus has a zero probability chance of being used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep approach 1 - Bidirectional LSTM with trainable Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Embedding, BatchNormalization, Flatten, Bidirectional, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a continuous list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(remove=['headers', 'footers'])\n",
    "text = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = ' '.join([' '.join(re.findall('(?u)\\\\b[a-zA-Z]*\\\\b',article.lower())) for article in text]).split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integer encode sequences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_data)\n",
    "sequences = tokenizer.texts_to_sequences(all_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into X and y sequences\n",
    "* X contains sequences of length `seq_length` of the previous words\n",
    "* y is the next word\n",
    "* For example:\n",
    "    * if the sentence is `i saw a dog on the street`, and `seq_length` = 3, we have\n",
    "    * X = 'i saw a', 'saw a dog', 'a dog on', ....\n",
    "    * y = 'dog', 'on', 'the' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data= []\n",
    "y_data = []\n",
    "seq_length=10\n",
    "for i in tqdm(range(len(sequences)-seq_length)):\n",
    "    X_data.append(sequences[i:i+seq_length])\n",
    "    y_data.append(sequences[i+seq_length])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work out the vocab list and size of vocab\n",
    "* Words are assigned values from 1 to the total number of words (e.g. 10,000). The Embedding layer needs to allocate a vector representation for each word in this vocabulary from index 1 to the largest index and because indexing of arrays is zero-offset, the index of the word at the end of the vocabulary will be 10,000; that means the array must be 10,000 + 1 in length.\n",
    "\n",
    "Therefore, when specifying the vocabulary size to the Embedding layer, we specify it as 1 larger than the actual vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(tokenizer.word_index.keys())\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape the X and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_data).reshape(len(X_data), seq_length)\n",
    "y = to_categorical(y_data) #onehot encode our y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=seq_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True), merge_mode='sum'))\n",
    "model.add(LSTM(128))\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=f\"best_weights.hdf5\"\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15, min_delta=0.0001) \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "callbacks = [early_stop, checkpoint]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=1, batch_size=128, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If trained, load weights from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a saved model\n",
    "# filename = \"weights_01.hdf5\"\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example\n",
    "* For a given input string generate some new text\n",
    "* the input string has to be pre-prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(seed_input):\n",
    "    \"\"\"prepare a string for the LSTM\"\"\"\n",
    "    \n",
    "    seed_input = seed_input.split()\n",
    "    try:\n",
    "        return np.expand_dims(np.array([tokenizer.word_index[x] for x in seed_input]),axis=0)\n",
    "    except:\n",
    "        return 'please try with a different sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_string, sentence_length):\n",
    "    \"\"\"generate some new text as a string\"\"\"\n",
    "    \n",
    "    seed = prepare_input(input_string)\n",
    "    for i in range(sentence_length-10):\n",
    "    #predict next word based on window of 10 previous words - and add to embedded doc\n",
    "        next_word = np.argmax(model.predict(seed[:,i:])).reshape(1,-1)\n",
    "        seed = np.append(seed,next_word,axis=1)\n",
    "\n",
    "    return ' '.join([tokenizer.index_word[x] for x in seed[0,:]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlike the traditional method, our seed has to be 10 words not 1 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = 'it was a dark and stormy night in berlin because'\n",
    "assert len(input_string.split()) == 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(input_string, sentence_length=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep approach 2 - Pretrained GPT model from OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_openai_text(prompt):\n",
    "    \"\"\"generate text from a pre-trained LLM from OpenAI\"\"\"\n",
    "    openai.api_key = os.getenv('OPENAI_KEY') \n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        temperature=0.7 #this is quite creative, set to 0.1 for vanilla, or 1 for highly creative\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_input = \"Once upon a time\"\n",
    "generated_text = generate_openai_text(user_input)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
