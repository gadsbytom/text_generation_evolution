{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models for text generation\n",
    "* One traditional approach\n",
    "* And one deep approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The objective is to create a model which tries to assess the liklehood of language:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w_{t+1} | w_{t-1+n}, ..., w_{t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional approach - trigram Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for a given dataset\n",
    "* bin the words into size 2 unique pairs\n",
    "* for a given pair, find the succeeding word\n",
    "* build a transition probability matrix (markov chain) of these relationships\n",
    "* use a sampler on this matrix to generate new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(remove=['headers', 'footers'])\n",
    "text = data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the trigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_word_distribution(data):\n",
    "    \"\"\"create a probability distribution over all trigrams\n",
    "    \n",
    "        params: data - a Bunch data object from sklearn\n",
    "        returns: [Bigram probability distribution, trigram probability distribution]\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = ' '.join([' '.join(re.findall('(?u)\\\\b\\\\w\\\\w*\\\\b',article.lower())) for article in text]).split()\n",
    "    tri_gram = [' '.join([x,y]) for x,y in zip(all_data[:-1:], all_data[1::])]\n",
    "    next_word = all_data[2:] + [' '] * 1\n",
    "    words = pd.DataFrame({'seed_word':all_data[:-1],'gram_words':tri_gram, \"next_word\":next_word})\n",
    "    words['seed_next_word'] = words['seed_word'].shift(-1)\n",
    "    seed_word_distribution = words.groupby('seed_word')['seed_next_word'].value_counts(normalize=True)\n",
    "    gram_word_distribution = words.groupby('gram_words')['next_word'].value_counts(normalize=True)\n",
    "    \n",
    "    return [seed_word_distribution, gram_word_distribution]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = trigram_word_distribution(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_text_generation(seed, length, distribution):\n",
    "    \"\"\"seed a distribution with a seed word, and ask it to make more words\n",
    "        \n",
    "        params: seed - A seed word, \n",
    "                length -Length of the generated sentence\n",
    "                distribution - A word probability distribution\n",
    "                \n",
    "        returns: generated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        seed = seed.lower()\n",
    "        seed += ' ' + np.random.choice(distribution[0][seed].index, p=distribution[0][seed].values)\n",
    "        for i in range(length):\n",
    "             seed += ' ' + np.random.choice(distribution[1][' '.join(seed.split()[-2:])].index, p=distribution[1][' '.join(seed.split()[-2:])].values)\n",
    "        return seed\n",
    "    \n",
    "    except:\n",
    "        print('Oops! Try another word as seed word')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'It'\n",
    "sentence_length = 20\n",
    "sentence = trigram_text_generation(seed,sentence_length, dist)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks:\n",
    "\n",
    "* It is still imperfect at capturing anything above basic syntax, and has no semantics or pragmatic capability\n",
    "* A quad-gram would be better, but as we increase gram size, transition matrices require increasingly more computation power to train, and space to store.\n",
    "* N-grams are a sparse representation of language -  any word not present in the training corpus has a zero probability chance of being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep approach - Bidirectional LSTM with trainable Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Embedding, BatchNormalization, Flatten, Bidirectional, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a continuous list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(remove=['headers', 'footers'])\n",
    "text = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = ' '.join([' '.join(re.findall('(?u)\\\\b[a-zA-Z]*\\\\b',article.lower())) for article in text]).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integer encode sequences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_data)\n",
    "sequences = tokenizer.texts_to_sequences(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into X and y sequences\n",
    "* X contains sequences of length `seq_length` of the previous words\n",
    "* y is the next word\n",
    "* For example:\n",
    "    * if the sentence is `i saw a dog on the street`, and `seq_length` = 3, we have\n",
    "    * X = 'i saw a', 'saw a dog', 'a dog on', ....\n",
    "    * y = 'dog', 'on', 'the' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data= []\n",
    "y_data = []\n",
    "seq_length=10\n",
    "for i in tqdm(range(len(sequences)-seq_length)):\n",
    "    X_data.append(sequences[i:i+seq_length])\n",
    "    y_data.append(sequences[i+seq_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work out the vocab list and size of vocab\n",
    "* Words are assigned values from 1 to the total number of words (e.g. 10,000). The Embedding layer needs to allocate a vector representation for each word in this vocabulary from index 1 to the largest index and because indexing of arrays is zero-offset, the index of the word at the end of the vocabulary will be 10,000; that means the array must be 10,000 + 1 in length.\n",
    "\n",
    "Therefore, when specifying the vocabulary size to the Embedding layer, we specify it as 1 larger than the actual vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(tokenizer.word_index.keys())\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape the X and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_data).reshape(len(X_data), seq_length)\n",
    "y = to_categorical(y_data) #onehot encode our y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=seq_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True), merge_mode='sum'))\n",
    "model.add(LSTM(128))\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=f\"best_weights.hdf5\"\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15, min_delta=0.0001) \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "callbacks = [early_stop, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=1, batch_size=128, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If trained, load weights from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a saved model\n",
    "# filename = \"weights_01.hdf5\"\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example\n",
    "* For a given input string generate some new text\n",
    "* the input string has to be pre-prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(seed_input):\n",
    "    \"\"\"prepare a string for the LSTM\"\"\"\n",
    "    \n",
    "    seed_input = seed_input.split()\n",
    "    try:\n",
    "        return np.expand_dims(np.array([tokenizer.word_index[x] for x in seed_input]),axis=0)\n",
    "    except:\n",
    "        return 'please try with a different sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_string, sentence_length):\n",
    "    \"\"\"generate some new text as a string\"\"\"\n",
    "    \n",
    "    seed = prepare_input(input_string)\n",
    "    for i in range(sentence_length-10):\n",
    "    #predict next word based on window of 10 previous words - and add to embedded doc\n",
    "        next_word = np.argmax(model.predict(seed[:,i:])).reshape(1,-1)\n",
    "        seed = np.append(seed,next_word,axis=1)\n",
    "\n",
    "    return ' '.join([tokenizer.index_word[x] for x in seed[0,:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlike the traditional method, our seed has to be 10 words not 1 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = 'it was a dark and stormy night in berlin because'\n",
    "assert len(input_string.split()) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(input_string, sentence_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
